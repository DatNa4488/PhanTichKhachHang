Beginning AutoGluon training... Time limit = 600s
AutoGluon will save models to 'D:\retail-demand-forecast\models\autogluon_forecast'
=================== System Info ===================
AutoGluon Version:  1.5.0
Python Version:     3.11.4
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          16
Pytorch Version:    2.9.1+cpu
CUDA Version:       CUDA is not available
GPU Memory:         
Total GPU Memory:   Free: 0.00 GB, Allocated: 0.00 GB, Total: 0.00 GB
GPU Count:          0
Memory Avail:       0.73 GB / 7.42 GB (9.8%)
Disk Space Avail:   131.41 GB / 195.31 GB (67.3%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MAE,
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 3,
 'prediction_length': 4,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 600,
 'verbosity': 2}

Frequency 'W' stored as 'W-SUN'
Beginning AutoGluon training... Time limit = 600s
AutoGluon will save models to 'D:\retail-demand-forecast\models\autogluon_forecast'
=================== System Info ===================
AutoGluon Version:  1.5.0
Python Version:     3.11.4
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.26100
CPU Count:          16
Pytorch Version:    2.9.1+cpu
CUDA Version:       CUDA is not available
GPU Memory:         
Total GPU Memory:   Free: 0.00 GB, Allocated: 0.00 GB, Total: 0.00 GB
GPU Count:          0
Memory Avail:       0.80 GB / 7.42 GB (10.7%)
Disk Space Avail:   131.41 GB / 195.31 GB (67.3%)
===================================================
Setting presets to: medium_quality

Fitting with arguments:
{'enable_ensemble': True,
 'eval_metric': MAE,
 'freq': 'W-SUN',
 'hyperparameters': 'light',
 'known_covariates_names': [],
 'num_val_windows': 3,
 'prediction_length': 4,
 'quantile_levels': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],
 'random_seed': 123,
 'refit_every_n_windows': 1,
 'refit_full': False,
 'skip_model_selection': False,
 'target': 'target',
 'time_limit': 600,
 'verbosity': 2}

Provided train_data has 142353 rows (NaN fraction=39.0%), 3665 time series. Median time series length is 49 (min=1, max=54). 
	Removing 640 short time series from train_data. Only series with length >= 17 will be used for training.
	After filtering, train_data has 137799 rows (NaN fraction=39.3%), 3025 time series. Median time series length is 52 (min=17, max=54). 

Provided data contains following columns:
	target: 'target'

AutoGluon will gauge predictive performance using evaluation metric: 'MAE'
	This metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.
===================================================

Starting training. Start time is 2026-01-22 14:14:43
Models that will be trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'Chronos2', 'TemporalFusionTransformer']
Training timeseries model SeasonalNaive. Training for up to 72.7s of the 581.3s of remaining time.
	-53.1991      = Validation score (-MAE)
	26.91   s     = Training runtime
	1.78    s     = Validation (prediction) runtime
Training timeseries model RecursiveTabular. Training for up to 78.9s of the 552.6s of remaining time.
	-53.3615      = Validation score (-MAE)
	4.80    s     = Training runtime
	0.20    s     = Validation (prediction) runtime
Training timeseries model DirectTabular. Training for up to 91.3s of the 547.5s of remaining time.
	-62.6647      = Validation score (-MAE)
	4.03    s     = Training runtime
	0.39    s     = Validation (prediction) runtime
Training timeseries model ETS. Training for up to 108.6s of the 543.1s of remaining time.
	Warning: ETS\W0 failed for 33 time series (1.1%). Fallback model SeasonalNaive was used for these time series.
	-47.4903      = Validation score (-MAE)
	9.17    s     = Training runtime
	2.56    s     = Validation (prediction) runtime
Training timeseries model Theta. Training for up to 132.8s of the 531.3s of remaining time.
	-47.2884      = Validation score (-MAE)
	4.41    s     = Training runtime
	1.87    s     = Validation (prediction) runtime
Training timeseries model Chronos2. Training for up to 175.0s of the 525.1s of remaining time.
	-41.4313      = Validation score (-MAE)
	34.82   s     = Training runtime
	13.21   s     = Validation (prediction) runtime
Training timeseries model TemporalFusionTransformer. Training for up to 238.5s of the 477.0s of remaining time.
	-41.9232      = Validation score (-MAE)
	231.12  s     = Training runtime
	3.12    s     = Validation (prediction) runtime
Fitting 1 ensemble(s), in 1 layers.
Training ensemble model WeightedEnsemble. Training for up to 242.4s.
	Ensemble weights: {'Chronos2': 0.52, 'ETS': 0.08, 'TemporalFusionTransformer': 0.36, 'Theta': 0.04}
	-40.7530      = Validation score (-MAE)
	5.11    s     = Training runtime
	20.76   s     = Validation (prediction) runtime
Training complete. Models trained: ['SeasonalNaive', 'RecursiveTabular', 'DirectTabular', 'ETS', 'Theta', 'Chronos2', 'TemporalFusionTransformer', 'WeightedEnsemble']
Total runtime: 349.79 s
Best model: WeightedEnsemble
Best model score: -40.7530
